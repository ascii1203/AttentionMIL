{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistBags(data_utils.Dataset):\n",
    "    def __init__(self, mean_bag_length=10, var_bag_length=2,\n",
    "                num_bag=250, seed=1, train=True):\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.train = train\n",
    "        \n",
    "        self.r = np.random.RandomState(seed)\n",
    "        \n",
    "        self.num_in_train = 60000\n",
    "        self.num_in_test = 10000\n",
    "        \n",
    "        if self.train:\n",
    "            self.train_bags_list, self.train_labels_list = self._create_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._create_bags()\n",
    "        \n",
    "    \n",
    "    def _create_bags(self):\n",
    "        if self.train:\n",
    "            loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                         train=True, download=True,\n",
    "                                                         transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,),(0.3081,))\n",
    "                        ])), batch_size=self.num_in_train, shuffle=False)\n",
    "        else:\n",
    "            loader = data_utils.DataLoader(datasets.MNIST('../datasets',\n",
    "                                                         train=False, download=True,\n",
    "                                                         transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,),(0.3081,))\n",
    "                        ])), batch_size=self.num_in_test, shuffle=False)\n",
    "        \n",
    "        for (batch_data, batch_labels) in loader:\n",
    "            all_imgs = batch_data\n",
    "            all_labels = batch_labels\n",
    "            print(all_imgs.shape)\n",
    "            print(all_labels.shape)\n",
    "            \n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for i in range(self.num_bag):\n",
    "            bag_length = np.int(self.r.normal(self.mean_bag_length, self.var_bag_length, 1))\n",
    "            if bag_length < 1:\n",
    "                bag_length = 1\n",
    "            \n",
    "            if self.train:\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_train, bag_length))\n",
    "            else:\n",
    "                indices = torch.LongTensor(self.r.randint(0, self.num_in_test, bag_length))\n",
    "            \n",
    "            labels_in_bag = all_labels[indices]\n",
    "            labels1 = torch.sum(labels_in_bag == 9) # True or False\n",
    "            labels2 = torch.sum(labels_in_bag == 8) # True or False'\n",
    "            \n",
    "            if labels1 > 0 and labels2 > 0 :\n",
    "                label = torch.FloatTensor([1,1,0])\n",
    "            elif labels1 > 0 and labels2 == 0 :\n",
    "                label = torch.FloatTensor([1,0,0])\n",
    "            elif labels1 == 0 and labels2 > 0:\n",
    "                label = torch.FloatTensor([0,1,0])\n",
    "            else:\n",
    "                label = torch.FloatTensor([0,0,1])\n",
    "            \n",
    "            \n",
    "            bags_list.append(all_imgs[indices])\n",
    "            labels_list.append(label)\n",
    "        return bags_list, labels_list\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = self.train_labels_list[index]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = self.test_labels_list[index]\n",
    "\n",
    "        return bag, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "train_loader = data_utils.DataLoader(MnistBags(\n",
    "    mean_bag_length=10, var_bag_length=2,\n",
    "    num_bag=200, seed=1, train=True), batch_size=1, shuffle=True)\n",
    "\n",
    "test_loader = data_utils.DataLoader(MnistBags(\n",
    "    mean_bag_length=10, var_bag_length=2,\n",
    "    num_bag=50, seed=1, train=False), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 500\n",
    "        self.D = 128\n",
    "        self.K = 1\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, 50 * 4 * 4)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL  i.e. 1x500\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob\n",
    "\n",
    "    #def calculate_classification_error(self,X,Y):\n",
    "    \n",
    "    #def calculate_objective(self,X,Y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 1.8987\n",
      "Epoch : 1, Loss : 1.8052\n",
      "Epoch : 2, Loss : 1.7416\n",
      "Epoch : 3, Loss : 1.5637\n",
      "Epoch : 4, Loss : 1.3572\n",
      "Epoch : 5, Loss : 1.0965\n",
      "Epoch : 6, Loss : 0.9558\n",
      "Epoch : 7, Loss : 0.7184\n",
      "Epoch : 8, Loss : 0.5702\n",
      "Epoch : 9, Loss : 0.4610\n",
      "Epoch : 10, Loss : 0.3579\n",
      "Epoch : 11, Loss : 0.2022\n",
      "Epoch : 12, Loss : 0.2069\n",
      "Epoch : 13, Loss : 0.3348\n",
      "Epoch : 14, Loss : 0.1342\n",
      "Epoch : 15, Loss : 0.0671\n",
      "Epoch : 16, Loss : 0.0571\n",
      "Epoch : 17, Loss : 0.0178\n",
      "Epoch : 18, Loss : 0.0113\n",
      "Epoch : 19, Loss : 0.0066\n",
      "Epoch : 20, Loss : 0.0048\n",
      "Epoch : 21, Loss : 0.0033\n",
      "Epoch : 22, Loss : 0.0024\n",
      "Epoch : 23, Loss : 0.0019\n",
      "Epoch : 24, Loss : 0.0016\n",
      "Epoch : 25, Loss : 0.0013\n",
      "Epoch : 26, Loss : 0.0011\n",
      "Epoch : 27, Loss : 0.0010\n",
      "Epoch : 28, Loss : 0.0008\n",
      "Epoch : 29, Loss : 0.0007\n",
      "Epoch : 30, Loss : 0.0006\n",
      "Epoch : 31, Loss : 0.0005\n",
      "Epoch : 32, Loss : 0.0005\n",
      "Epoch : 33, Loss : 0.0005\n",
      "Epoch : 34, Loss : 0.0004\n",
      "Epoch : 35, Loss : 0.0004\n",
      "Epoch : 36, Loss : 0.0004\n",
      "Epoch : 37, Loss : 0.0003\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "model = Attention()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0005, betas=(0.9,0.999), weight_decay=10e-5)\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_error = 0\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        predict = model(data)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(predict[0][0],label[0][0]) + F.binary_cross_entropy(predict[0][1],label[0][1]) + F.binary_cross_entropy(predict[0][2],label[0][2])\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    print('Epoch : {}, Loss : {:.4f}'.format(i, train_loss))\n",
    "#data = torch.zeros([13,1,28,28])\n",
    "#label = torch.torch.LongTensor([1,0,0])\n",
    "#model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 3.1674, 39/50 Correct\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "num_samples = 0\n",
    "num_correct = 0\n",
    "\n",
    "for batch_idx, (data,label) in enumerate(test_loader):\n",
    "    predict = model(data)\n",
    "    loss = F.binary_cross_entropy(predict[0][0],label[0][0]) + F.binary_cross_entropy(predict[0][1],label[0][1]) + F.binary_cross_entropy(predict[0][2],label[0][2])\n",
    "    test_loss += loss.item()\n",
    "    if torch.equal(torch.ge(predict[0],0.7).float(), label[0]):\n",
    "        num_correct += 1\n",
    "    num_samples += 1\n",
    "    \n",
    "test_loss /= len(test_loader)       \n",
    "print('Test Loss : {:.4f}, {}/{} Correct'.format(test_loss,num_correct,num_samples))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
